{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Sampling & Reliability\n",
    "\n",
    "Up until this week, we have assumed that the corpus you have used for analysis assignments represented a *meaningful* assemblage of texts from which reasonable inferences could be drawn about the social game, social world and social actors that produced it. This week, we ask you to articulate what your sample represents in context of your final project, and draw upon methods we have developed over the last three weeks to draw reasonable and representative samples. \n",
    "\n",
    "This week we also pivot from our recent work on unsupervized machine learning approaches to explore how we can get *human* readings of content at scale. We want to gather and utilize human responses for several reasons. First, we may want to use crowdsourced human scores as the primary method of coding, extracting or organizing content (as it was in the two of the assigned readings). Second, we may want to validate or tune a computational algorithm we may have developed in terms of how it is associated with human meanings or experience. Finally, we may want to use human coding on a sample of data as the basis for training a model or algorithm to then extrapolate *human-like* annotations to the entire population. Here intelligent sampling is critical to maximize effective maching training. \n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import numpy as np #For arrays\n",
    "import scipy as sp #For some stats\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import pyanno #On python3 make sure to pip install pyanno3\n",
    "import nltk\n",
    "\n",
    "#We need to import these this way due to how pyanno is setup\n",
    "from pyanno.measures import pairwise_matrix, agreement, cohens_kappa, cohens_weighted_kappa, fleiss_kappa, krippendorffs_alpha, pearsons_rho, scotts_pi, spearmans_rho\n",
    "from pyanno.annotations import AnnotationsContainer\n",
    "from pyanno.models import ModelA, ModelBt, ModelB\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import permutations\n",
    "import math\n",
    "\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Pitch Your Project*</span>\n",
    "\n",
    "<span style=\"color:red\">In the three cells immediately following, describe **WHAT** you are planning to analyze for your final project (i.e., texts, contexts and the social game, world and actors you intend to learn about through your analysis) (<200 words), **WHY** you are going to do it (i.e., why would theory and/or the average person benefit from knowing the results of your investigation) (<200 words), and **HOW** you plan to investigate it (i.e., what are the approaches and operations you plan to perform, in sequence, to yield this insight) (<400 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***What?*** \n",
    "\n",
    "\n",
    "I plan to investigate and compare samples from two corpora related to the literature on early linguistic development. First, I will work with a corpus of the text from 100 popular children's picture books sourced from [Montag et al., 2015](http://journals.sagepub.com/doi/abs/10.1177/0956797615594361). Also, I will work on recorded transcipt corpora from the [Language Development Project](https://ldp.uchicago.edu/) based at UChicago; this project has collected a huge database of recorded and transcribed conversations between caregivers and their children at various stages of development, providing a wealth of information about the content of child-produced and child-directed speech throughout linguistic development. Through this project I hope to use these two corpora to examine substantive differences in syntactical content, potentially illuminating the important role played by books on early language learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Why?***\n",
    "\n",
    "While, within the developmental literature, much has been studied about the content of child-direct and -exposed speech, less work has focused on the content of children's books, with even less directly comparing the two kinds of linguistic input. This kind of analysis is notably absent in the wake of widely-publicized projects concerning the impact of language exposure (through conversation & written text) on developmental outcomes (e.g., [TMW Initiative](http://thirtymillionwords.org/tmw-initiative/)). Through my project, I would contribute to discussions surrounding the influence of children's literature on linguistic development by comparing the relative content/contributions of child-directed speech and popular children's picture books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***How?***\n",
    "\n",
    "At this stage, most of what I am interested in pertains to corpus linguistics in each sample set. For example, I am interested in replicating some work (e.g., [Montag et al., 2015](http://journals.sagepub.com/doi/abs/10.1177/0956797615594361), [Montag et al, 2017](https://psyarxiv.com/4p8r2/)) examining word type and token statistics between similarly-sized books and child-directed speech samples. I'd like to investigate more in this vein; I am curious to examine whether certain parts of speech or syntactical components appear more or less often in one corpus compared to another. Relative discrepancies would indicate particular roles played by each kind of input in contributing to lingustic development, pointing towards a mechanism by which development might be assessed / improved among at-risk populations. \n",
    "\n",
    "In order to conduct this kind of investigation, I aim to make heavy use of stemming and lemmatizing procedures as covered in Week 2 of the course. This will be particularly useful in my investigation of word type and token statistics. Since I am interested in potential discrepancies in parts-of-speech as well, I will want to make heavy use of the POS tagging and parsing techiques to be covered in Week 7. I haven't had much luck with clustering/topic modeling/word embedding models in previous weeks' assignments, but potentially, with some fine tuning, they could reveal some insights about how samples from the two corpora differ in ways more abstract than the word level. Finally, if I do find there to be substantive differences between the content in the two corpora, it could be enlightening/fun to train a classifier to categorize extracts based on POS, word types, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Pitch Your Sample*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cell immediately following, describe the rationale behind your proposed sample design for your final project. What is the social game, social work, or social actors you about whom you are seeking to make inferences? What are its virtues with respect to your research questions? What are its limitations? What are alternatives? What would be a reasonable path to \"scale up\" your sample for further analysis (i.e., high-profile publication)? (<200 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Which (words)?***\n",
    "\n",
    "As previously mentioned, my two corpora are a set of 100 children's picture books and a set of transcribed conversations between parents and children between the ages of 14 - 58 months sourced from the [Language Development Project](https://ldp.uchicago.edu/). Since the latter dataset is much larger than the former, I will need to do some subsampling in order to yield balanced subsets of content and thus make legitimate comparsons. I aim to follow the procedure implemented in [Montag et al., 2015](http://journals.sagepub.com/doi/abs/10.1177/0956797615594361), wherein, for each book in the corpus, an equally-sized contiguous excerpt was drawn from the conversation corpus ([CHILDES](https://childes.talkbank.org/) in this case), after which progressively larger samples of words were taken and analyzed from each sample set. This entire process was run 100 times; on each instance, each book was paired with a different length-matched contiguous excerpt from the conversation corpus. By sampling and subsampling in this way, I would allow for direct comparison between the two corpus types; importantly, taking contiguous samples from the conversation database maintains dependency relations between the words within the sample, making the conversation sample akin to a book in that they are both consistent at the topic level. \"Scaling up\" this sample would involve transcribing more books in order to allow for more robust comparisons between that corpus and the myriad of child-directed speech corpora that exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, demonstrate three approaches drawn from any of the last three weeks to create subsamples of your corpus (e.g., for crowd-sourced subcoding like we'll be doing this week). Methods of subsampling include (a) corpus linguistic approaches drawing on word presence or co-presence, conditional frequencies, weighted words (e.g., tf.idf), KL or JS divergences, etc.; (b) hierarchical or flat clustering approaches; (c) topic modeling; or (d) word-embedding. (<200 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldpdf = feather.read_dataframe('../../../../ldp-alignment/utterances.feather')\n",
    "bookdf = pandas.read_csv('../ChildBookProject/bookdf.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookdf['tokenized_sents'] = bookdf['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "      <th>line</th>\n",
       "      <th>p_chat</th>\n",
       "      <th>c_chat</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>play with Mommy's hand .</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>no .</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>no, no, no .</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>nope no no .</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>yeah, she likes my phone .</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  session  line                      p_chat c_chat  order\n",
       "0       22        1   8.0    play with Mommy's hand .             1\n",
       "1       22        1   9.0                        no .             2\n",
       "2       22        1  10.0                no, no, no .             3\n",
       "3       22        1  11.0                nope no no .             4\n",
       "4       22        1  13.0  yeah, she likes my phone .             5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldpdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldpdf22 = ldpdf[ldpdf['subject']==22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ldpdf['all_chat'] = ldpdf.apply(lambda row: row['p_chat']+ row['c_chat'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jules Feiffer</td>\n",
       "      <td>George’s mother said: \"Bark, George.\" George w...</td>\n",
       "      <td>Bark, George</td>\n",
       "      <td>[[George, ’, s, mother, said, :, ``, Bark, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert McCloskey</td>\n",
       "      <td>One day, Little Sal went with her mother to Bl...</td>\n",
       "      <td>Blueberries for Sal</td>\n",
       "      <td>[[One, day, ,, Little, Sal, went, with, her, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bill Martin Jr</td>\n",
       "      <td>Brown Bear, Brown Bear, What do you see? I see...</td>\n",
       "      <td>Brown Bear, Brown Bear, What Do You See?</td>\n",
       "      <td>[[Brown, Bear, ,, Brown, Bear, ,, What, do, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doreen Cronin</td>\n",
       "      <td>Farmer Brown has a problem. His cows like to t...</td>\n",
       "      <td>Click, Clack, Moo Cows that Type</td>\n",
       "      <td>[[Farmer, Brown, has, a, problem, .], [His, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Judi Barrett</td>\n",
       "      <td>We were all sitting around the big kitchen tab...</td>\n",
       "      <td>Cloudy With a Chance of Meatballs</td>\n",
       "      <td>[[We, were, all, sitting, around, the, big, ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                                               text  \\\n",
       "0      Jules Feiffer  George’s mother said: \"Bark, George.\" George w...   \n",
       "1   Robert McCloskey  One day, Little Sal went with her mother to Bl...   \n",
       "2     Bill Martin Jr  Brown Bear, Brown Bear, What do you see? I see...   \n",
       "3      Doreen Cronin  Farmer Brown has a problem. His cows like to t...   \n",
       "4       Judi Barrett  We were all sitting around the big kitchen tab...   \n",
       "\n",
       "                                      title  \\\n",
       "0                              Bark, George   \n",
       "1                       Blueberries for Sal   \n",
       "2  Brown Bear, Brown Bear, What Do You See?   \n",
       "3          Click, Clack, Moo Cows that Type   \n",
       "4         Cloudy With a Chance of Meatballs   \n",
       "\n",
       "                                     tokenized_sents  \n",
       "0  [[George, ’, s, mother, said, :, ``, Bark, ,, ...  \n",
       "1  [[One, day, ,, Little, Sal, went, with, her, m...  \n",
       "2  [[Brown, Bear, ,, Brown, Bear, ,, What, do, yo...  \n",
       "3  [[Farmer, Brown, has, a, problem, .], [His, co...  \n",
       "4  [[We, were, all, sitting, around, the, big, ki...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookdf[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one way of sampling from the book corpus based solely on word occurrence. For example, here are all the sentences in the book corpus that include the word 'little.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "littlesents = []\n",
    "for i, r in bookdf.iterrows():\n",
    "    for s in r['tokenized_sents']:\n",
    "        if 'little' in s:\n",
    "            littlesents.append(' '.join(s))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Little Sal picked three berries and dropped them in her little tin pail .',\n",
       " 'We ran downstairs for breakfast and ate it a little faster than usual so we could go sledding with Grandpa .']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "littlesents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a random sentence from each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randsents = []\n",
    "for i, r in bookdf.iterrows():\n",
    "    val = np.random.randint(0, len(r['tokenized_sents'])-1)\n",
    "    randsents.append(' '.join(r['tokenized_sents'][val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"George went : `` Moo . ''\",\n",
       " \"Mother wants to take her berries home and can them for next winter . ''\",\n",
       " 'Brown Bear , Brown Bear , What do you see ?',\n",
       " 'No eggs .',\n",
       " 'The job was too big .']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randsents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turkrandsents = randsents[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkranddf = pandas.DataFrame({'sents':turkrandsents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George went : `` Moo . ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mother wants to take her berries home and can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown Bear , Brown Bear , What do you see ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No eggs .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The job was too big .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Then one morning a little girl stopped and loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The man picked him up quickly and popped him i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>He grew until he was two years old , and he ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>His mother said , `` I ’ m afraid it won ’ t c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The snow was piled up very high along the stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Then he looked beyond the thornbushes , out in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>He made some more windows .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>In my bouquet were gold , bread , and salt-and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I reached up and touched the top of it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Good night , Giraffe .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thank the lord you are well !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>And she did .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Yes , David .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Then he called back : `` Whoo-whoo-who-who-who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>She slept in the nest at night .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The winds blew , this way and that .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>And the tree was happy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I did not rustle the sheets .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>He was too jumpy !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>In the nights she watched the moon grow from a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>calls the Once-ler .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>`` Shucks , '' said the bunny , `` I might jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Who needs sixteen ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Camilla was so embarrassed .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I bet your mom would let me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A wakeful flea on a slumbering mouse on a snoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>`` I heard , '' he said , `` that you are goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>In the evening , after dinner , he tells the O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>And when he came to the place where the wild t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>They lived with their mother in a sandbank , u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>When he got to the middle of the ring he saw t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Down , out of the tree he went .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>She sprouted leaves and petals .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Maybe it ’ s the dancing .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>`` Shove on , Shorty ! ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>When he looks at the pictures , he ’ ll get so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Everyone is just waiting .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>And the Little Blue Engine smiled and seemed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>In the great green room there was a telephone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>But- chick chicka boom boom !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I can talk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Would you eat them with a fox ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>`` Sure , I cam use you ! ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>`` Finders , keepers , '' is what I say !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sents\n",
       "0                           George went : `` Moo . ''\n",
       "1   Mother wants to take her berries home and can ...\n",
       "2         Brown Bear , Brown Bear , What do you see ?\n",
       "3                                           No eggs .\n",
       "4                               The job was too big .\n",
       "5   Then one morning a little girl stopped and loo...\n",
       "6   The man picked him up quickly and popped him i...\n",
       "7   He grew until he was two years old , and he ra...\n",
       "8   His mother said , `` I ’ m afraid it won ’ t c...\n",
       "9   The snow was piled up very high along the stre...\n",
       "10  Then he looked beyond the thornbushes , out in...\n",
       "11                        He made some more windows .\n",
       "12  In my bouquet were gold , bread , and salt-and...\n",
       "13           I reached up and touched the top of it .\n",
       "14                             Good night , Giraffe .\n",
       "15                      Thank the lord you are well !\n",
       "16                                      And she did .\n",
       "17                                      Yes , David .\n",
       "18  Then he called back : `` Whoo-whoo-who-who-who...\n",
       "19                   She slept in the nest at night .\n",
       "20               The winds blew , this way and that .\n",
       "21                           And the tree was happy .\n",
       "22                      I did not rustle the sheets .\n",
       "23                                 He was too jumpy !\n",
       "24  In the nights she watched the moon grow from a...\n",
       "25                               calls the Once-ler .\n",
       "26  `` Shucks , '' said the bunny , `` I might jus...\n",
       "27                                Who needs sixteen ?\n",
       "28                       Camilla was so embarrassed .\n",
       "29                      I bet your mom would let me .\n",
       "30  A wakeful flea on a slumbering mouse on a snoo...\n",
       "31  `` I heard , '' he said , `` that you are goin...\n",
       "32  In the evening , after dinner , he tells the O...\n",
       "33  And when he came to the place where the wild t...\n",
       "34  They lived with their mother in a sandbank , u...\n",
       "35  When he got to the middle of the ring he saw t...\n",
       "36                   Down , out of the tree he went .\n",
       "37                   She sprouted leaves and petals .\n",
       "38                         Maybe it ’ s the dancing .\n",
       "39                          `` Shove on , Shorty ! ''\n",
       "40  When he looks at the pictures , he ’ ll get so...\n",
       "41                         Everyone is just waiting .\n",
       "42  And the Little Blue Engine smiled and seemed t...\n",
       "43  In the great green room there was a telephone ...\n",
       "44                      But- chick chicka boom boom !\n",
       "45                                       I can talk .\n",
       "46                    Would you eat them with a fox ?\n",
       "47                       `` Sure , I cam use you ! ''\n",
       "48          `` Finders , keepers , '' is what I say !"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turkranddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turkranddf.to_csv('turkranddf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Annotation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Rzhetsky et al (2009)'s sample dataset, which can be found [here](https://github.com/enthought/uchicago-pyanno/tree/master/data). This data is the result of a content analytic / content extraction study in which Andrey Rzhetsky and colleagues from the National Library of Medicine, published [here](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000391) in [PLOS Computational Biology](http://journals.plos.org/ploscompbiol/), gave eight annotators 10,000 sentence chunks from biomedical text in biomedical abstracts and articles, then asked them, in a loop design schematically illustrated below that provided 3 independent codings for each document. The sampling strategy pursued diversity by drawing from PubMed abstracts (1000) and full-text articles (9000: 20% from abstracts, 10% from introductions, 20% from methods, 25% from results, and 25% from discussions.) The dataset extract here involves respondents codes for sentences in terms of their *Evidence*: {0, 1, 2, 3, -1} where 0 is the complete lack of evidence, 3 is direct evidence present within the sentence, and -1 is didn't respond. (They also crowdsourced and analyzed *polarity*, *certainty*, and *number*). For example, consider the following two abutting sentence chunks: *\"Because null mutations in toxR and toxT abolish CT and TcpA expression in the El Tor biotype and also attenuate virulence...\"* [i.e., average certainty = 0], *\"...it is likely that the ToxR regulon has functional similarities between the two biotypes despite the clear differences in the inducing parameters observed in vitro\"* [i.e., average certainty = 1].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.loadtxt(\"../data/pyAnno/testdata_numerical.txt\")\n",
    "anno = AnnotationsContainer.from_array(x, missing_values=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate the AnnotationsContainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3, ..., -1, -1, -1],\n",
       "       [ 0,  0,  0, ..., -1, -1, -1],\n",
       "       [ 2,  2,  1, ..., -1, -1, -1],\n",
       "       ...,\n",
       "       [ 2,  2, -1, ..., -1, -1,  1],\n",
       "       [ 2,  2, -1, ..., -1, -1,  3],\n",
       "       [ 1,  1, -1, ..., -1, -1,  0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we assume categorical codes...that each code is qualitatively distinct from each other. Two measures are primarily used for this: Scott's $\\pi$, Cohen's $\\kappa$, and Krippendorff's $\\alpha$ which each measure the extent of agreement between two annotators, but take into account the possibility of the agreement occurring by chance in slightly different ways. Any agreement measure begins with the frequency of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11666667, 0.245     , 0.34083333, 0.2975    ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyanno.measures.agreement.labels_frequency(anno.annotations,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the \"confusion matrix\" or matrix of coded agreements between any two coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.  7.  3.  2.]\n",
      " [ 9. 30. 13.  3.]\n",
      " [ 2.  9. 42. 11.]\n",
      " [ 1.  7. 14. 39.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD8CAYAAAA2Y2wxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f0a5240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = pyanno.measures.agreement.confusion_matrix(anno.annotations[:,0], anno.annotations[:,1],4)\n",
    "print(c)\n",
    "ac = seaborn.heatmap(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scott's $\\pi$ is computed as:\n",
    "\n",
    "$\\pi = \\frac{\\text{Pr}(a)-\\text{Pr}(e)}{1-\\text{Pr}(e)}$\n",
    "\n",
    "Where Pr($a$) is relative observed agreement, and Pr($e$) is expected agreement using joint proportions calculated from the confusion matrix or matrix of coded agreements between any two coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4386478265891502"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scotts_pi(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of Scott's $\\pi$ to $n$ coders is Fleiss' $\\kappa$ (Fleiss called it $\\kappa$ because he thought he was generalizing Cohen's $\\kappa$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40417653771912765"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleiss_kappa(anno.annotations[::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krippendorff's $\\alpha$ generalizes of Fleiss' $\\kappa$ to $n$ coders and takes into account the fact that annotations here are not categorically different, but ordinal, by adding a weight matrix in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When produced with no arguments, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "krippendorffs_alpha(anno.annotations[::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Scott's $\\pi$, Cohen's $\\kappa$ also takes into account the possibility of the agreement occurring by chance, but in the following way:\n",
    "\n",
    "$\\kappa = \\frac{p_o-p_e}{1-p_e}=1-\\frac{1-p_o}{p_e}$\n",
    "\n",
    "where $p_o$ is the relative observed agreement among raters, and $p_e$ is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in complete agreement then $\\kappa = 1$. If there is no agreement among the raters other than what would be expected by chance (as given by $p_e$), $\\kappa ≤ 0 $. Here, Cohen's $\\kappa$ statistic for the first two annotators is computed. This is probably the most common metric of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cohens_kappa(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = pairwise_matrix(cohens_kappa, anno.annotations)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this 8 by 3 loop design will be less stable than an 8 choose 3 combinatorial design, because each codes with more others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also assess the average Cohen's $\\kappa$ for all pairs of coders that have coded against one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_metric_average(metric, array):\n",
    "    \"\"\"Calculate the pairwise metric average for the real elements of metric function run on an array of annotations\"\"\"\n",
    "    p = permutations(range(array[0,:].size),2)\n",
    "    m = [metric(array[:,x[0]], array[:,x[1]]) for x in p]\n",
    "    clean_m = [c for c in m if not math.isnan(c)]\n",
    "    return reduce(lambda a, b: a + b, clean_m)/len(clean_m)    \n",
    " \n",
    "pairwise_metric_average(cohens_kappa, anno.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As recognized with Krippendorff's flexible $\\alpha$, our scores are *not* categorical, but rather ordered and her considered metric. Weighted $\\kappa$ allows you to count disagreements differently and is useful when codes are ordered as they are here. Here a weight matrix is added to the calculation, in which off-diagonal cells contain weights indicating the seriousness of the disagreement between each score. When automatically produced, it simply produces an arithmetic distance (e.g., 3-1=2), such that cells one off the diagonal are weighted 1, two off 2, etc. Here\n",
    "\n",
    "$\\kappa = 1-\\frac{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}x_{ij}}{\\sum^k_{i=1}\\sum^k_{j=1}w_{ij}m_{ij}}$\n",
    "\n",
    "where $\\kappa$ = $n$ codes and $w_{ij}$,$x_{ij}$, and $m_{ij}$ represent elements in the weight, observed, and expected matrices, respectively. (Obviously, when diagonal cells contain weights of 0 and off-diagonal cells weights of 1, this equals $\\kappa$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cohens_weighted_kappa(anno.annotations[:,0], anno.annotations[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or averaged over the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairwise_metric_average(cohens_weighted_kappa,anno.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if the annontation data can be understood as indicating real values, we can assess not agreement, but rather the correlation of values (Pearson's $\\rho$) or correlation of ranks (Spearman's $\\rho$) for pairs of coders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = pairwise_matrix(pearsons_rho, anno.annotations)\n",
    "m = pairwise_matrix(spearmans_rho, anno.annotations)\n",
    "an = seaborn.heatmap(n)\n",
    "plt.show()\n",
    "am = seaborn.heatmap(m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or averaged over all comparable pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pairwise_metric_average(pearsons_rho,anno.annotations), pairwise_metric_average(spearmans_rho,anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Excercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">Perform a content annotation survey of some kind in which at least 3 people evaluate and code each piece of content, using Amazon Mechanical Turk as described in the MTurk slides on this week's Canvas page.  With the resulting data, calculate, visualize and discuss inter-coder agreement or covariation with appropriate metrics. What does this means for the reliability of human assessments regarding content in your domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my practice content annotation survey, I picked 50 random sentences from my book corpus and had Turk workers perform a sentiment judgement (on a 5 point Likert scale.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if some coders are better than others. The prior measures all rely on the assumption that all coders are equally good. What if some are worse than others? Now we use Rzhetsky et al (2009) and Dawid & Skene's models to make inference about true label classes by downweighting bad or deviant coders. Pyanno provides two relevant models: ModelB and ModelBt. Model B with $\\theta$s models the relationship between each coder and code. Model B is a Bayesian generalization of the Dawid & Skene model from the reading. The following image schematically describes these models. <img src=\"../data/pyanno/Bmodel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models should provide very similar results. To estimate the parameters for any models, we first need to create a new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new instance of model B, with 4 label classes\n",
    "model = ModelB.create_initial_state(4,8)\n",
    "# other model parameters are initialized from the model prior\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on our data. Pyanno allows one to use either MAP (maximum a posteriori estimation) or MLE (maximum likelihood estimation) to estimate model parameters. Note that the parameters here correspond to our estimation of the accuracy of each annotator-annotation pair. First we will train with MAP, and then you can optionally training with MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.map(anno.annotations)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below if you would rather train by MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = ModelB.create_initial_state(4,8)\n",
    "#model.mle(anno.annotations)\n",
    "#print(model.theta)\n",
    "#print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have model parameters estimated, we can now make inferences about the true label classes. We can calculate the posterior distribution over the true label classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posterior = model.infer_labels(anno.annotations)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn the posterior of the first 100 samples into a heatmap and compare with the raw vote tallies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeQuestionComparison(model, anno_target, num_questions = 20):\n",
    "    votes = []\n",
    "    for r in anno_target.annotations:\n",
    "        v = [0] * len(anno_target.labels)\n",
    "        votes.append(v)\n",
    "        for a in r:\n",
    "            if a > -1:\n",
    "                v[a] += 1\n",
    "    votes_array = np.array(votes)\n",
    "    posterior = model.infer_labels(anno_target.annotations)\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (15, 10), sharey=True)\n",
    "\n",
    "    seaborn.heatmap(votes_array[:num_questions], annot = True, ax=ax2)\n",
    "    seaborn.heatmap(np.nan_to_num(posterior,0)[:num_questions], annot=True, ax =ax1)\n",
    "    ax1.set_title(\"Model\")\n",
    "    ax2.set_title(\"Votes\")\n",
    "    return fig, (ax1, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeQuestionComparison(model, anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This differs markedly from taking annotator scores at face value. As you can see (the model-based probabilities are on left; the raw votes on right), the models helps us break ties between coders and in some cases dramatically downgrades the estimates from particularly bad coders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try everything again with ModelBt, which constrains ModelB so that each coder has a single estimate over the accuracy of all of their codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new instance of model B, with 4 label classes and 8 annotators.\n",
    "model = ModelBt.create_initial_state(4, 8)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.map(anno.annotations)\n",
    "print(model.theta)\n",
    "print(model.log_likelihood(anno.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posterior = model.infer_labels(anno.annotations)\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the posterior of the first 10 samples according to ModelBt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeQuestionComparison(model, anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property of these scores is that they enable us to identify the most likely code assuming coders of unequal quality, which also allows us to break ties when we know coder identity. We may also use the posterior themselves rather than the most probably code outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing coder accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot of the accuracy for each annotator inferred from ModelBt. This might be helpful if you wanted to ban a \"troll\" from your crowdsourcing enterprise. The coder a very low accuracy likelihood is an outlier and should be \"dropped from the team.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ModelBt.create_initial_state(4,8)\n",
    "model.mle(anno.annotations)\n",
    "samples = model.sample_posterior_over_accuracy(anno.annotations, 200, burn_in_samples=100, thin_samples=3)\n",
    "y =  samples.mean(axis=0)\n",
    "y_ci = samples.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(range(8),y, yerr = y_ci)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do a simpler analysis we can look at a different data set, here are some hotel reviews from [expedia](https://www.kaggle.com/c/expedia-hotel-recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hotels = pandas.read_csv('../data/hot_Reviews.csv', index_col=0)\n",
    "df_hotels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a rank of 0 is a missing value and to simplify things more we will convert from a 1-10 scale to a 1-5 scale, with 0 as missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hotels = df_hotels.apply(lambda x: x // 2) #integer divide by 2 rounds all values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize all the reviews as a heatmap with the missing values greyed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,20))\n",
    "seaborn.heatmap(df_hotels, cmap='rainbow', ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give the dataframe to pyanno we need to convert to np array and change the nans to intergers, lets use -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hot_mat = np.array(df_hotels.fillna(-1).as_matrix())\n",
    "anno_hot = AnnotationsContainer.from_array(hot_mat, missing_values=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_hot.annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_hot.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_hot.missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at coder agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyanno.measures.agreement.labels_frequency(anno_hot.annotations, 6)#6 possible catagories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = pyanno.measures.agreement.confusion_matrix(anno_hot.annotations[:,0], anno_hot.annotations[:,1], 6) #6 possible catagories\n",
    "print(c)\n",
    "ac = seaborn.heatmap(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most agreement is on 2 i.e. an average hotel and there's little agreement as rating go higher, likely due to scarcity in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scotts_pi(anno_hot.annotations[:,0], anno_hot.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "krippendorffs_alpha(anno_hot.annotations[::])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cohens_kappa(anno_hot.annotations[:,0], anno_hot.annotations[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = pairwise_matrix(cohens_kappa, anno_hot.annotations)\n",
    "fig, ax = plt.subplots(figsize = (15, 15))\n",
    "seaborn.heatmap(m, ax =ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to look at what model Bt thinkgs the correct reviews are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_hot = ModelBt.create_initial_state(6, 49)\n",
    "model_hot.mle(anno_hot.annotations)\n",
    "#print(model.theta)\n",
    "print(model_hot.log_likelihood(anno_hot.annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "makeQuestionComparison(model_hot, anno_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the results of your content annotation survey to predict high and low-quality analysts, then predict MLE or MAP estimates for your codes in question. What do these estimates suggest about the distribution of skill among your coders? How different are these estimates from a majority vote?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
